{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acuBg2hBfftz"
      },
      "outputs": [],
      "source": [
        "!pip install gradio rake-nltk PyPDF2 pymupdf4llm nltk groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJSh8Yo9fKrq",
        "outputId": "cb0c683b-ccaf-496d-94ae-36e53862d781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import pymupdf4llm\n",
        "import numpy as np\n",
        "import re\n",
        "from rake_nltk import Rake\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mil-obMWfSlA"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess jobs data\n",
        "jobs_data = pd.read_csv(\"/content/Jobs_data_new_python.csv\")\n",
        "jobs_data = jobs_data.dropna(subset=[\"Job_txt\", \"job-title\"]).reset_index(drop=True)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text.lower()\n",
        "\n",
        "jobs_data[\"Job_txt_cleaned\"] = jobs_data[\"Job_txt\"].apply(preprocess_text)\n",
        "job_descriptions_cleaned = jobs_data[\"Job_txt_cleaned\"].tolist()\n",
        "job_titles = jobs_data[\"job-title\"].tolist()\n",
        "\n",
        "# Initialize Rake\n",
        "rake = Rake()\n",
        "\n",
        "def extract_keywords(text):\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    return rake.get_ranked_phrases()\n",
        "\n",
        "def calculate_base_similarity(cv_keywords, job_descriptions):\n",
        "    cv_text = \" \".join(cv_keywords)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(job_descriptions + [cv_text])\n",
        "    return cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "\n",
        "def calculate_interest_similarity(interest, job_titles):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(job_titles + [interest])\n",
        "    return cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "\n",
        "def calculate_soft_skill_similarity(soft_skills, job_descriptions):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(job_descriptions + [soft_skills])\n",
        "    return cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "\n",
        "def combine_scores(base_scores, interest_scores, soft_skills_scores,\n",
        "                   interest_weight=0.4, soft_skills_weight=0.2):\n",
        "    base_weight = 1 - interest_weight - soft_skills_weight\n",
        "    return (interest_weight * interest_scores) + \\\n",
        "           (soft_skills_weight * soft_skills_scores) + \\\n",
        "           (base_weight * base_scores)\n",
        "\n",
        "def process_inputs(cv_file, interests, soft_skills):\n",
        "    # Extract text from CV\n",
        "    cv_markdown = pymupdf4llm.to_markdown(cv_file, page_chunks=True)\n",
        "    cv_text = \" \".join([chunk['text'] for chunk in cv_markdown])\n",
        "\n",
        "    # Extract keywords\n",
        "    cv_keywords = extract_keywords(cv_text)\n",
        "\n",
        "    # Calculate similarities\n",
        "    base_scores = calculate_base_similarity(cv_keywords, job_descriptions_cleaned)\n",
        "    print(\"Base scores:\", base_scores)\n",
        "\n",
        "    interest_scores = calculate_interest_similarity(interests, job_titles)\n",
        "\n",
        "    if soft_skills.strip():\n",
        "        soft_skills_scores = calculate_soft_skill_similarity(soft_skills, job_descriptions_cleaned)\n",
        "    else:\n",
        "        soft_skills_scores = np.zeros_like(base_scores)\n",
        "\n",
        "    # Combine scores\n",
        "    combined_scores = combine_scores(base_scores, interest_scores, soft_skills_scores)\n",
        "\n",
        "    # Create results dataframe\n",
        "    results = jobs_data.copy()\n",
        "    results[\"Match Score\"] = combined_scores\n",
        "    results = results.sort_values(\"Match Score\", ascending=False).head(10)\n",
        "\n",
        "    return results[[\"job-title\", \"company\", \"location\", \"Match Score\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxPZW94lhFbo"
      },
      "outputs": [],
      "source": [
        "cv_file=\"/content/Ismail-Oubah-EnglishCV.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roSBtL6BhFlG"
      },
      "outputs": [],
      "source": [
        "interests=\"Machine Learning, Data Analysis, Cloud Computing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBpVmoIvhW_8"
      },
      "outputs": [],
      "source": [
        "soft_skills=\"Team Leadership, Public Speaking, Time Management\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvoKXhOphFYH",
        "outputId": "dd875649-8e09-4587-9858-e4d97f37254e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/Ismail-Oubah-EnglishCV.pdf...\n",
            "[                                        ] (0/1)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b========================================\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[========================================] (1/1)\b\b\b\b\b\b\b]\n",
            "Base scores: [0.01420772 0.04080659 0.01419871 0.04896231 0.01819197 0.01503125\n",
            " 0.01442724 0.01227876 0.05480118 0.00577657 0.03463781 0.01183512\n",
            " 0.00809821 0.02057038 0.02845185 0.04673173 0.00299636 0.01385939\n",
            " 0.02438939 0.01420772 0.04080659 0.01419871 0.04896231 0.01819197\n",
            " 0.01503125 0.01442724 0.01227876 0.05480118 0.00577657 0.03463781\n",
            " 0.01183512 0.00809821 0.02057038 0.02845185 0.04673173 0.00299636\n",
            " 0.01385939 0.02438939 0.02286805 0.01694891 0.02269844 0.02205852\n",
            " 0.02152584 0.03221842 0.0681525  0.01163456 0.06814935 0.06735825\n",
            " 0.06736248 0.01898978 0.02685288 0.02817086 0.01239296 0.04000192\n",
            " 0.05173441 0.01309528 0.01308111 0.00889626 0.08768648 0.01891137\n",
            " 0.00631102 0.03593707 0.03613241 0.0239789  0.03061403 0.05211745\n",
            " 0.00838116 0.02368204 0.01814852 0.01446327 0.09361828 0.01610436\n",
            " 0.0059946  0.0215342  0.01960281 0.0681525  0.0681525  0.0681525\n",
            " 0.0681525  0.07032398 0.02311389 0.01138298 0.02378224 0.16849664\n",
            " 0.03358973 0.01093584 0.01538656 0.12190402 0.02881679 0.04586248\n",
            " 0.03493189 0.03312047 0.03420861 0.0301645  0.01751603 0.04817321\n",
            " 0.09732467 0.00582638 0.02787701 0.02499221 0.02630447 0.06736248\n",
            " 0.01733432 0.06783711 0.02018812 0.0124557  0.02425875 0.01467732\n",
            " 0.00761318 0.00673672 0.01779699 0.00505415 0.01253666 0.01931269\n",
            " 0.02272806 0.01099965 0.0055691  0.01562552 0.04567956 0.05405648\n",
            " 0.04366254 0.01863828 0.02373977 0.06736248 0.06736248 0.06736248\n",
            " 0.01506053 0.00961762 0.02936713 0.02001149 0.03251646 0.04140614\n",
            " 0.02966371 0.04453427 0.01055034 0.04241703 0.01272371 0.0075291\n",
            " 0.01013559 0.02119726 0.00382704 0.03944959 0.0128279  0.04994648\n",
            " 0.01786657 0.02023392 0.04345546 0.10462746 0.02374098 0.0862123\n",
            " 0.06736174 0.06736248 0.06736248 0.06736248 0.06736248 0.06736248\n",
            " 0.06736248 0.06736248 0.01021421 0.01064879 0.04023197 0.03794128\n",
            " 0.01215799 0.01258286 0.02309179 0.00541756 0.06287461 0.03198215\n",
            " 0.0160161  0.01415653 0.02135382 0.10463423 0.04239546 0.03878314\n",
            " 0.06736174 0.06736174 0.06736174 0.06736174 0.06736174 0.06736174\n",
            " 0.06736174 0.06736174 0.06736174 0.06736174 0.01793495 0.05682353\n",
            " 0.05272297 0.03335787 0.01780086 0.01074574 0.00441633 0.00902613\n",
            " 0.0121957  0.00836086 0.03934919 0.01610462 0.06441058 0.0187598\n",
            " 0.04345825 0.02589878 0.06736174 0.06736174 0.06736174 0.07083748\n",
            " 0.02374098 0.03878486 0.01138867 0.01621372 0.01246332 0.05189511\n",
            " 0.0373418  0.00485369 0.01596047 0.06960176 0.0191004  0.03871728\n",
            " 0.00476006 0.02849362 0.02468376 0.0173147  0.03334026 0.00812084\n",
            " 0.07193907 0.00673062 0.02411983 0.02437805 0.02374098 0.0259842\n",
            " 0.10463423 0.10463423 0.02023603 0.0115569  0.01185251 0.04447473\n",
            " 0.00674442 0.03238798 0.00589758 0.04020607 0.02257056 0.01402012\n",
            " 0.04172154 0.01711916 0.02533991 0.03151519 0.07060779 0.06522536\n",
            " 0.0397862  0.01738149 0.02825889 0.04345825 0.02676773 0.02374098\n",
            " 0.10463423 0.10463423 0.03878486 0.03878486 0.01309528 0.02160985\n",
            " 0.03562638 0.05619019 0.04491175 0.02414711 0.00985064 0.0181506\n",
            " 0.00819178 0.00860599 0.00776562 0.05213976 0.00837633]\n"
          ]
        }
      ],
      "source": [
        "result=process_inputs(cv_file, interests, soft_skills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wM6HFcS3fVfn",
        "outputId": "9a29b704-3203-4d6b-e3ce-ffe4fd127f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a6ba274a50a9c9ad43.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a6ba274a50a9c9ad43.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /tmp/gradio/357f7f813d5bc7d71a03e37db89c8318ece06936054ebba7191fe84df76752d4/Ismail-Oubah-EnglishCV.pdf...\n",
            "[                                        ] (0/1)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b========================================\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[========================================] (1/1)\b\b\b\b\b\b\b]\n",
            "Base scores: [0.01420772 0.04080659 0.01419871 0.04896231 0.01819197 0.01503125\n",
            " 0.01442724 0.01227876 0.05480118 0.00577657 0.03463781 0.01183512\n",
            " 0.00809821 0.02057038 0.02845185 0.04673173 0.00299636 0.01385939\n",
            " 0.02438939 0.01420772 0.04080659 0.01419871 0.04896231 0.01819197\n",
            " 0.01503125 0.01442724 0.01227876 0.05480118 0.00577657 0.03463781\n",
            " 0.01183512 0.00809821 0.02057038 0.02845185 0.04673173 0.00299636\n",
            " 0.01385939 0.02438939 0.02286805 0.01694891 0.02269844 0.02205852\n",
            " 0.02152584 0.03221842 0.0681525  0.01163456 0.06814935 0.06735825\n",
            " 0.06736248 0.01898978 0.02685288 0.02817086 0.01239296 0.04000192\n",
            " 0.05173441 0.01309528 0.01308111 0.00889626 0.08768648 0.01891137\n",
            " 0.00631102 0.03593707 0.03613241 0.0239789  0.03061403 0.05211745\n",
            " 0.00838116 0.02368204 0.01814852 0.01446327 0.09361828 0.01610436\n",
            " 0.0059946  0.0215342  0.01960281 0.0681525  0.0681525  0.0681525\n",
            " 0.0681525  0.07032398 0.02311389 0.01138298 0.02378224 0.16849664\n",
            " 0.03358973 0.01093584 0.01538656 0.12190402 0.02881679 0.04586248\n",
            " 0.03493189 0.03312047 0.03420861 0.0301645  0.01751603 0.04817321\n",
            " 0.09732467 0.00582638 0.02787701 0.02499221 0.02630447 0.06736248\n",
            " 0.01733432 0.06783711 0.02018812 0.0124557  0.02425875 0.01467732\n",
            " 0.00761318 0.00673672 0.01779699 0.00505415 0.01253666 0.01931269\n",
            " 0.02272806 0.01099965 0.0055691  0.01562552 0.04567956 0.05405648\n",
            " 0.04366254 0.01863828 0.02373977 0.06736248 0.06736248 0.06736248\n",
            " 0.01506053 0.00961762 0.02936713 0.02001149 0.03251646 0.04140614\n",
            " 0.02966371 0.04453427 0.01055034 0.04241703 0.01272371 0.0075291\n",
            " 0.01013559 0.02119726 0.00382704 0.03944959 0.0128279  0.04994648\n",
            " 0.01786657 0.02023392 0.04345546 0.10462746 0.02374098 0.0862123\n",
            " 0.06736174 0.06736248 0.06736248 0.06736248 0.06736248 0.06736248\n",
            " 0.06736248 0.06736248 0.01021421 0.01064879 0.04023197 0.03794128\n",
            " 0.01215799 0.01258286 0.02309179 0.00541756 0.06287461 0.03198215\n",
            " 0.0160161  0.01415653 0.02135382 0.10463423 0.04239546 0.03878314\n",
            " 0.06736174 0.06736174 0.06736174 0.06736174 0.06736174 0.06736174\n",
            " 0.06736174 0.06736174 0.06736174 0.06736174 0.01793495 0.05682353\n",
            " 0.05272297 0.03335787 0.01780086 0.01074574 0.00441633 0.00902613\n",
            " 0.0121957  0.00836086 0.03934919 0.01610462 0.06441058 0.0187598\n",
            " 0.04345825 0.02589878 0.06736174 0.06736174 0.06736174 0.07083748\n",
            " 0.02374098 0.03878486 0.01138867 0.01621372 0.01246332 0.05189511\n",
            " 0.0373418  0.00485369 0.01596047 0.06960176 0.0191004  0.03871728\n",
            " 0.00476006 0.02849362 0.02468376 0.0173147  0.03334026 0.00812084\n",
            " 0.07193907 0.00673062 0.02411983 0.02437805 0.02374098 0.0259842\n",
            " 0.10463423 0.10463423 0.02023603 0.0115569  0.01185251 0.04447473\n",
            " 0.00674442 0.03238798 0.00589758 0.04020607 0.02257056 0.01402012\n",
            " 0.04172154 0.01711916 0.02533991 0.03151519 0.07060779 0.06522536\n",
            " 0.0397862  0.01738149 0.02825889 0.04345825 0.02676773 0.02374098\n",
            " 0.10463423 0.10463423 0.03878486 0.03878486 0.01309528 0.02160985\n",
            " 0.03562638 0.05619019 0.04491175 0.02414711 0.00985064 0.0181506\n",
            " 0.00819178 0.00860599 0.00776562 0.05213976 0.00837633]\n"
          ]
        }
      ],
      "source": [
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as interface:\n",
        "    gr.Markdown(\"# 🧑💼 Job Recommendation System\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            cv_upload = gr.File(label=\"Upload your CV (PDF)\", file_types=[\".pdf\"])\n",
        "            interests_input = gr.Textbox(\n",
        "                label=\"Your Professional Interests\",\n",
        "                placeholder=\"e.g., Machine Learning, Data Analysis, Cloud Computing\"\n",
        "            )\n",
        "            soft_skills_input = gr.Textbox(\n",
        "                label=\"Your Soft Skills (Optional)\",\n",
        "                placeholder=\"e.g., Team Leadership, Public Speaking, Time Management\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Find Matching Jobs\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            results_table = gr.DataFrame(\n",
        "                label=\"Recommended Jobs\",\n",
        "                headers=[\"Job Title\", \"Company\", \"Location\", \"Match Score\"],\n",
        "                datatype=[\"str\", \"str\", \"str\", \"number\"]\n",
        "            )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_inputs,\n",
        "        inputs=[cv_upload, interests_input, soft_skills_input],\n",
        "        outputs=results_table\n",
        "    )\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYI3MRJfV4b0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ8bEuUNV-sx"
      },
      "source": [
        "##Test with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svDbJ0z4V5Mi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcoLF5DSV5P6"
      },
      "outputs": [],
      "source": [
        "prompt_cv=\"\"\"\n",
        "\n",
        "You are a highly intelligent assistant tasked with analyzing CVs and creating concise, structured summaries optimized for comparing with job descriptions.\n",
        "\n",
        "Your goal is to extract and organize key details into specific categories. Structure your response as follows:\n",
        "\n",
        "1. **Professional Summary**: Provide a three-sentences max overview of the candidate’s career focus, expertise, and achievements.\n",
        "2. **Key Skills and Expertise**: Enumerate the candidate's main skills, including technical skills (e.g., programming languages, tools,frameworks, services) and non-technical skills (e.g., leadership, communication).\n",
        "3. **Work Experience**:\n",
        "   - For each role, include:\n",
        "     - Job Title.\n",
        "     - Organization Name.\n",
        "     - Duration of Employment.\n",
        "     - Key responsibilities and accomplishments.\n",
        "4. **Technologies and Tools**: List every software, programming languages, frameworks,API service, and tools the candidate has experience with.\n",
        "5. **Education**: Summarize degrees, fields of study, and institutions attended.\n",
        "6. **Certifications and Training**: Highlight any certifications, courses, or training programs completed.\n",
        "7. **Languages**: Mention languages the candidate knows and their proficiency levels.\n",
        "8. **Projects**: Include significant projects or achievements relevant to the candidate's profile.\n",
        "9. **Keywords**: Extract important keywords that characterize the candidate’s expertise.\n",
        "\n",
        "Focus on clarity and precision in each section to ensure a well-structured summary. Here's the CV content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Respond in the requested structured format.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSpNXMKdWh3b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TMW-tAjWh66"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY=\"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\"\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "def Summarize_job_text(content,prompt):\n",
        "    \"\"\"\n",
        "    Summarize a table using an LLM.\n",
        "    \"\"\"\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"you are a helpful assistant.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt.format(content=content),\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=0.2,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYCRPSDZWxMz",
        "outputId": "3edd5c2e-501f-4e67-a96b-db8b08590154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/Ismail-Oubah-EnglishCV.pdf...\n",
            "[                                        ] (0/1)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b========================================\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[========================================] (1/1)\b\b\b\b\b\b\b]\n"
          ]
        }
      ],
      "source": [
        "cv_file=\"/content/Ismail-Oubah-EnglishCV.pdf\"\n",
        "cv_markdown = pymupdf4llm.to_markdown(cv_file, page_chunks=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYjtTPX1WxGk"
      },
      "outputs": [],
      "source": [
        "cv_text = \" \".join([chunk['text'] for chunk in cv_markdown])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "DQblpaaOWxJV",
        "outputId": "cda4ea82-666b-4604-f96c-49279cd15d2a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## AI developer\\n\\n[oubah.ismail.07@gmail.com](mailto:oubah.ismail.07@gmail.com)\\n\\n[+212628873435](tel:+212628873435)\\n\\n[https://www.fiverr.com/ismailobh](https://www.fiverr.com/ismailobh)\\n\\n[github.com/ismailox1000](https://github.com/ismailox1000)\\n\\nhttps://huggingface.co/obh07\\n\\n\\n### EDUCATION\\n\\n**Master Degree in Computer Engineering,**\\nIstanbul Aydin University\\n2022 – 2024 | Istanbul, Turkey\\n\\n - hesis About \"Advanced RAG Multilingual\\nSemantic Retrieval across Document Types\\nby Finetuning Transformer Based Language\\nModels and OCR Integration”.\\n\\n**Bachelor in Maintenance of Embedded**\\n**Electronic Systems, EST**\\n2020 – 2021 | Salé, Morocco\\n\\n**Technical University degree, EST**\\n2018 – 2020 | Salé, Morocco\\n\\n### TECHNICAL SKILLS\\n\\n**Programming Languages : Python,**\\nJavaScript, SQL, HTML, CSS, Bootstrap\\n**Data Analysis & Data Science : Pandas,**\\nNumPy, Scikit-Learn, Matplotlib, Seaborn,\\nOpenCV, Power BI\\n**Machine Learning : Régression,**\\nClassification, Clustering\\n**Deep Learning : PyTorch, TensorFlow,**\\nANN, CNN, RNN, LSTM, OCR,NLP,\\nTransformers\\n**Databases : MySQL, PostgreSQL, VectorDB**\\n,FAISS Web Frameworks & Automation :\\nFlask, Streamlit, Gradio,Selenium,\\nBeautifulSoup, Scrapy\\n\\n### SOFT SKILLS\\n\\nProblem Solver - Communication Teamwork - Time Management - Creativity\\n\\n### LANGUAGES\\n\\nArabic\\n\\n\\n**Outlier, RLHF Remote Tasker**\\n10/2024 – present\\n\\n - valuated and enhanced AI models to align with human preferences.\\n\\n - rovided detailed feedback to improve language generation quality.\\n\\n - ontributed to developing more natural AI systems through rigorous\\nassessments.\\n\\n**conseil ingénierie et développement (CID), AI developer (Intern)**\\n12/2023 – 07/2024 | Technopolis, Morocco\\n\\n - utomated document processing tasks (rotation, merging, compression, and\\nmapping) and multilingual OCR.\\n\\n - reated synthetic datasets to fine-tune a custom similarity search model.\\n\\n - uantized LLAMA 3 model for use on limited GPU resources.\\n\\n - uilt a multilingual question-answering system (RAG) with PostgreSQL,\\nfeaturing keyword and similarity-based search.\\n\\n - eveloped an interactive web application using Flask, Bootstrap, and\\nJavaScript.\\n\\n**Freelancer, Fiverr**\\n03/2023 – 11/2023\\n\\n - eveloped custom AI solutions, including web applications and chatbots.\\n\\n - erformed data extraction using Python, Scrapy, and Selenium.\\n\\n - elivered projects on time and within budget, ensuring effective client\\nmanagement.\\n\\n### PERSONAL PROJECTS\\n\\n**Document Manipulation Application (In Development),**\\nDeveloping a comprehensive application for document processing with:\\n\\n - lassic Tools: Image extraction, watermarking, PDF encryption, and\\ncompression.\\n\\n - I Tools: PDF interaction (LLAMA 3 via Groq API), summarization,\\ntranslation with format preservation, and table extraction from both standard\\nand scanned PDFs.\\n\\n - ackend developed using Flask, with a user interface built with Bootstrap and\\nJavaScript.\\n\\n**Order Management System for Cafés and Restaurants (In Development),**\\nQR code-based ordering system with:\\n\\n - able-side order placement and automated notifications for customers and\\nbaristas.\\n\\n - ashboard for visualizing and analyzing sales, orders, and additional features.\\n\\n - eveloped using Flask, Bootstrap, and JavaScript.\\n\\n**Job Scraper, Matcher and Recommender**\\n\\n - mplemented job scraping, matching, and recommendation systems using\\nBeautifulSoup, Selenium.\\n\\n - eveloped automatic cover letter generation using generative AI.\\n\\n**Multilingual AI Chatbots**\\n\\n - uilding Chatbot web applications and for Telegram and WhatsApp using\\nLangChain, LLAMAIndex, and OpenAI.\\n\\n - everaging HuggingFace models such as LLAMA 3 and APIs such as Groq\\nAPI for fast inferencing and many more .\\n\\n**Alzheimer\\'s disease Classification**\\n\\n - esigned a web application with Python and Streamlit for classifying stages\\nof Alzheimer\\'s disease.\\n\\n**Corn plant diseases Classification**\\n\\n - uilding a classification model for maize plant diseases using a convolutional\\nneural network (CNN).\\n\\n**Facial Recognition System**\\n\\n - uilding a custom Deep Learning model for facial recognition using the\\nLabelMe library and OpenCV.\\n\\n**Machine learning Models**\\n\\n - eveloped ML models for regression, classification, and clustering using\\n\\n\\n-----\\n\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI3KujK9WlRD"
      },
      "outputs": [],
      "source": [
        "test2=Summarize_job_text(cv_text,prompt_cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69U_5laOXsSZ",
        "outputId": "263c1d64-970e-46d4-ed11-be1770abdf39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.16)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.32)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.16 marshmallow-3.26.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Tt5VABWv-n"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import JinaEmbeddings, HuggingFaceEmbeddings\n",
        "from pathlib import Path\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "def create_vectorstore( model=\"jina\", db_name=\"job_matching_db\", jina_api_key=None):\n",
        "    \"\"\"\n",
        "    Create a vector store from a list of documents for similarity search.\n",
        "\n",
        "    Args:\n",
        "        documents (list of str): List of text documents to embed and store.\n",
        "        model (str): Embedding model to use ('jina' or 'e5').\n",
        "        db_name (str): Name of the database for storage.\n",
        "        jina_api_key (str, optional): API key for Jina embeddings.\n",
        "\n",
        "    Returns:\n",
        "        Chroma: Initialized vectorstore.\n",
        "    \"\"\"\n",
        "    # Define available embedding models\n",
        "    embedding_models = {\n",
        "        \"e5\": \"obh07/multilingual-e5-base-dolly-15k\",\n",
        "        \"jina\": \"jina-embeddings-v3\"\n",
        "    }\n",
        "\n",
        "    # Select the embedding function\n",
        "    if model == \"e5\":\n",
        "        embedding_function = HuggingFaceEmbeddings(\n",
        "            model_name=embedding_models[\"e5\"],\n",
        "            show_progress=True\n",
        "        )\n",
        "    elif model == \"jina\" and jina_api_key:\n",
        "        embedding_function = JinaEmbeddings(\n",
        "            jina_api_key=jina_api_key,\n",
        "            model_name=embedding_models[\"jina\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model specified. Choose 'e5' or 'jina' with a valid API key for Jina.\")\n",
        "\n",
        "    # Set up persistence directory for the vectorstore\n",
        "    persist_directory = f\"./chroma_data_{db_name}\"\n",
        "    Path(persist_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize and populate the vectorstore\n",
        "    vectorstore = Chroma(\n",
        "        collection_name=f\"job_documents_{db_name}\",\n",
        "        embedding_function=embedding_function,\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def create_retriever(vectorstore):\n",
        "    store = InMemoryStore()\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    retriever = MultiVectorRetriever(\n",
        "      vectorstore=vectorstore,\n",
        "      docstore=store,\n",
        "      id_key=id_key,search_kwargs={\"k\": 10},\n",
        "  )\n",
        "    return retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw98U3hHX6lE"
      },
      "outputs": [],
      "source": [
        "jobs_data = pd.read_csv(\"/content/Jobs_data_new_python.csv\")\n",
        "jobs_data = jobs_data.dropna(subset=[\"Job_txt\", \"job-title\"]).reset_index(drop=True)\n",
        "json_data = jobs_data.to_dict(orient=\"records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKPkf9LUYi9C",
        "outputId": "625073b8-d494-4dbd-9ff9-d9b167e53448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Job_ID': 4041424674,\n",
              " 'Job_txt': 'Internship for Webmaster (M/F) (Stage pour Webmaster (H/F))\\nORIENTAL GROUP / Marrakech Casablanca, Casablanca-Settat, Morocco\\n1 month ago 124 applicants\\nSee who ORIENTAL GROUP / Marrakech has hired for this role\\nApply\\nJoin or sign in to find your next job\\nJoin to apply for the Internship for Webmaster (M/F) (Stage pour Webmaster (H/F)) role at ORIENTAL GROUP / Marrakech\\nNot you?\\nRemove photo\\nFirst name\\nLast name\\nEmail\\nPassword (6+ characters)\\nBy clicking Agree & Join, you agree to the LinkedIn User Agreement, Privacy Policy and Cookie Policy.\\nContinue Agree & Join\\nor\\nApply on company website\\nSecurity verification\\nAlready on LinkedIn? Sign in\\nSave\\nSave job\\nSave this job with your existing LinkedIn profile, or create a new one.\\nYour job seeking activity is only visible to you.\\nEmail\\nContinue\\nWelcome back\\nSign in to save Internship for Webmaster (M/F) (Stage pour Webmaster (H/F)) at ORIENTAL GROUP / Marrakech.\\nEmail or phone\\nPassword\\nShow\\nForgot password? Sign in\\nORIENTAL GROUP / Marrakech provided pay range\\nThis range is provided by ORIENTAL GROUP / Marrakech. Your actual pay will be based on your skills and experience — talk with your recruiter to learn more.\\nBase pay range\\nMAD 48,000.00/yr - MAD 90,000.00/yr\\nOur company BioProGreen, based in Marrakech for several years now, is a producer and supplier of natural essential oils, perfumes, and high-quality natural aromas in Morocco and worldwide. Our company\\'s mission is to meet the most demanding market requirements in terms of quality and delivery times while respecting ethical and environmental standards.\\n\\nThe Role\\n\\nYou Will Be Responsible For\\n\\nDesigning interfaces for websites that provide the best possible user experience.\\nTaking a \"design brief\" to understand requirements.\\nPresenting ideas, concepts and design solutions to various stakeholders.\\nIdentifying design problems and devising elegant solutions.\\n\\nIdeal Profile\\n\\nYou have at least 1 year experience including solid experience in a similar role within IT.\\nYou are a good multi-tasker who can work within tight deadlines.\\nYou possess excellent communication and interpersonal skills and can articulate your ideas to different stakeholders.\\nYou have working knowledge of web\\nYou are a strong networker & relationship builder\\nYou are a strong team player who can manage multiple stakeholders\\nYou are adaptable and thrive in changing environments\\nYou are willing to undertake 30-60% travel.\\n\\nWhat\\'s on Offer?\\n\\nA chance to accelerate your career\\nGreat work culture\\nOpportunity within company with a solid track record of success\\nShow more Show less\\nSeniority level\\nInternship\\nEmployment type\\nFull-time\\nJob function\\nEngineering and Information Technology\\nIndustries\\nSoftware Development\\nReferrals increase your chances of interviewing at ORIENTAL GROUP / Marrakech by 2x\\nSee who you know',\n",
              " 'company': 'ORIENTAL GROUP / Marrakech',\n",
              " 'job-title': 'Internship for Webmaster (M/F) (Stage pour Webmaster (H/F))',\n",
              " 'level': 'Internship',\n",
              " 'location': 'Casablanca, Casablanca-Settat, Morocco',\n",
              " 'posted-time-ago': '1 month ago',\n",
              " 'nb_candidats': 124.0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jp-3erDIZCPa",
        "outputId": "b2663e9f-137d-475c-f275-87e7ab86dc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.8)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.11.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.11.0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53771 sha256=866ce41c736f6247f47c92f532b0b973cad955f032a5d465f3cc22b7376cb51f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, importlib-metadata, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.5.0 kubernetes-32.0.0 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.11.0 protobuf-5.29.3 pypika-0.48.9 pyproject_hooks-1.2.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "73c4e9d5b33a4b1181cf42d61906401c",
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWpmogixYpzA"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "\n",
        "\n",
        "jina_api_key=\"jina_a336488c98c6463180a2494f62739952boWqK6nYZSwFNd-Z6JJJ-Gvs1sqW\"  # Replace with your API key\n",
        "\n",
        "vectorstore = create_vectorstore( model=\"jina\", db_name=\"job_matching_3\", jina_api_key=jina_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFJctYRUZP0g"
      },
      "outputs": [],
      "source": [
        "retriever=create_retriever(vectorstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgylJRV9ZXGo"
      },
      "outputs": [],
      "source": [
        "def add_documents_to_retriever(retriever, job_elements=None):\n",
        "    \"\"\"\n",
        "    Adds table and text documents to the retriever and its docstore.\n",
        "\n",
        "    Args:\n",
        "        retriever (MultiVectorRetriever): The retriever object.\n",
        "        job_elements (list, optional): A list of job elements to add, where each element is expected to be a dictionary.\n",
        "    \"\"\"\n",
        "    if job_elements:\n",
        "        # Generate unique doc IDs for job elements\n",
        "        job_doc_ids = [str(uuid.uuid4()) for _ in job_elements]\n",
        "\n",
        "        # Prepare documents with 'summary' as page_content and the rest as metadata\n",
        "        job_documents = []\n",
        "        for doc_id, job in zip(job_doc_ids, job_elements):\n",
        "            if isinstance(job, dict) and 'Job_txt' in job:\n",
        "                metadata = {k: v for k, v in job.items() if k != 'Job_txt'}  # Exclude 'summary' from metadata\n",
        "                metadata[\"doc_id\"] = doc_id  # Add the unique doc_id to metadata\n",
        "                job_documents.append(Document(page_content=job['Job_txt'], metadata=metadata))\n",
        "\n",
        "        # Add documents to retriever's vectorstore and docstore\n",
        "        retriever.vectorstore.add_documents(job_documents)\n",
        "        retriever.docstore.mset(list(zip(job_doc_ids, job_elements)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SCwq5j0Z9wH",
        "outputId": "c512bf31-75b8-47af-873e-c882ad439526"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "269"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eqcpyv8IZ7NX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        " # Ensure json_data is defined as a list\n",
        "json_data_sample = json_data[:150]\n",
        "add_documents_to_retriever(retriever, job_elements=json_data_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mxq6Ph1aIx4"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import JinaEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lInvpdIaaLg1"
      },
      "outputs": [],
      "source": [
        "embedding_model = JinaEmbeddings(\n",
        "    jina_api_key=\"jina_a336488c98c6463180a2494f62739952boWqK6nYZSwFNd-Z6JJJ-Gvs1sqW\",  # Replace with your API key\n",
        "    model_name=\"jina-embeddings-v3\"\n",
        ")\n",
        "cv_embedding = embedding_model.embed_query(test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT6cCZjYabxP",
        "outputId": "c7b6a71e-e7dc-4331-b8b7-34f9af87566f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity Score: 0.64\n"
          ]
        }
      ],
      "source": [
        "test=json_data[100]['Job_txt']\n",
        "\n",
        "job_embedding = embedding_model.embed_query(test)\n",
        "# Compute similarity\n",
        "similarity_score = cosine_similarity([job_embedding], [cv_embedding])[0][0]\n",
        "\n",
        "print(f\"Similarity Score: {similarity_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfk3A4KyboHZ"
      },
      "outputs": [],
      "source": [
        "#loop through retriever and calculate similarity to each data in it with the CV\n",
        "def get_matching_jobs(cv_embedding, retriever):\n",
        "    matching_jobs = []\n",
        "    for doc in retriever.vectorstore._collection.get().values():\n",
        "        job_embedding = doc.embedding\n",
        "        similarity_score = cosine_similarity([job_embedding], [cv_embedding])[0][0]\n",
        "        matching_jobs.append((doc.metadata, similarity_score))\n",
        "    return matching_jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pilLjdXscQ7H"
      },
      "outputs": [],
      "source": [
        "retriever.vectorstore._collection.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUo6NK6hahi9",
        "outputId": "b1fbb1d1-85aa-4aa4-b405-3bcbe4d03828"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Job_ID': 4041424674,\n",
              " 'Job_txt': 'Internship for Webmaster (M/F) (Stage pour Webmaster (H/F))\\nORIENTAL GROUP / Marrakech Casablanca, Casablanca-Settat, Morocco\\n1 month ago 124 applicants\\nSee who ORIENTAL GROUP / Marrakech has hired for this role\\nApply\\nJoin or sign in to find your next job\\nJoin to apply for the Internship for Webmaster (M/F) (Stage pour Webmaster (H/F)) role at ORIENTAL GROUP / Marrakech\\nNot you?\\nRemove photo\\nFirst name\\nLast name\\nEmail\\nPassword (6+ characters)\\nBy clicking Agree & Join, you agree to the LinkedIn User Agreement, Privacy Policy and Cookie Policy.\\nContinue Agree & Join\\nor\\nApply on company website\\nSecurity verification\\nAlready on LinkedIn? Sign in\\nSave\\nSave job\\nSave this job with your existing LinkedIn profile, or create a new one.\\nYour job seeking activity is only visible to you.\\nEmail\\nContinue\\nWelcome back\\nSign in to save Internship for Webmaster (M/F) (Stage pour Webmaster (H/F)) at ORIENTAL GROUP / Marrakech.\\nEmail or phone\\nPassword\\nShow\\nForgot password? Sign in\\nORIENTAL GROUP / Marrakech provided pay range\\nThis range is provided by ORIENTAL GROUP / Marrakech. Your actual pay will be based on your skills and experience — talk with your recruiter to learn more.\\nBase pay range\\nMAD 48,000.00/yr - MAD 90,000.00/yr\\nOur company BioProGreen, based in Marrakech for several years now, is a producer and supplier of natural essential oils, perfumes, and high-quality natural aromas in Morocco and worldwide. Our company\\'s mission is to meet the most demanding market requirements in terms of quality and delivery times while respecting ethical and environmental standards.\\n\\nThe Role\\n\\nYou Will Be Responsible For\\n\\nDesigning interfaces for websites that provide the best possible user experience.\\nTaking a \"design brief\" to understand requirements.\\nPresenting ideas, concepts and design solutions to various stakeholders.\\nIdentifying design problems and devising elegant solutions.\\n\\nIdeal Profile\\n\\nYou have at least 1 year experience including solid experience in a similar role within IT.\\nYou are a good multi-tasker who can work within tight deadlines.\\nYou possess excellent communication and interpersonal skills and can articulate your ideas to different stakeholders.\\nYou have working knowledge of web\\nYou are a strong networker & relationship builder\\nYou are a strong team player who can manage multiple stakeholders\\nYou are adaptable and thrive in changing environments\\nYou are willing to undertake 30-60% travel.\\n\\nWhat\\'s on Offer?\\n\\nA chance to accelerate your career\\nGreat work culture\\nOpportunity within company with a solid track record of success\\nShow more Show less\\nSeniority level\\nInternship\\nEmployment type\\nFull-time\\nJob function\\nEngineering and Information Technology\\nIndustries\\nSoftware Development\\nReferrals increase your chances of interviewing at ORIENTAL GROUP / Marrakech by 2x\\nSee who you know',\n",
              " 'company': 'ORIENTAL GROUP / Marrakech',\n",
              " 'job-title': 'Internship for Webmaster (M/F) (Stage pour Webmaster (H/F))',\n",
              " 'level': 'Internship',\n",
              " 'location': 'Casablanca, Casablanca-Settat, Morocco',\n",
              " 'posted-time-ago': '1 month ago',\n",
              " 'nb_candidats': 124.0}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CtuGWl9fKKs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18IUWogcfKNr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwEmUmcPfKQ8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_qX5bAifKT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrOaaSbNfKWv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWYdvP9JfKaP"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import pandas as pd\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import JinaEmbeddings\n",
        "from langchain.schema.document import Document\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Enhanced CV Analysis Prompt\n",
        "prompt_cv = PromptTemplate.from_template(\"\"\"\n",
        "You are an expert CV analyst. Extract and structure key information from this CV for optimal job matching:\n",
        "\n",
        "1. **Professional Summary**: [3-sentence career overview with quantifiable achievements]\n",
        "2. **Core Competencies**:\n",
        "   - Technical: [Programming languages, frameworks, tools]\n",
        "   - Soft Skills: [Leadership, communication, project management]\n",
        "3. **Experience**:\n",
        "   - For each role:\n",
        "     - Title: [Job title]\n",
        "     - Company: [Employer]\n",
        "     - Duration: [Dates]\n",
        "     - Achievements: [Quantified impact statements]\n",
        "4. **Education**: [Degrees, institutions, relevant coursework]\n",
        "5. **Certifications**: [Technical certifications with issuing bodies]\n",
        "6. **Key Projects**: [3-5 projects with technologies used and business impact]\n",
        "7. **Industry Keywords**: [15-20 technical terms and domain-specific concepts]\n",
        "\n",
        "Format response in Markdown with clear section headers. Focus on concrete, measurable details.\n",
        "\n",
        "CV Content:\n",
        "{content}\n",
        "\"\"\")\n",
        "\n",
        "# Enhanced Vector Store Setup\n",
        "def create_enhanced_vectorstore(job_data, embedding_model, persist_dir=\"job_chroma_db\"):\n",
        "    \"\"\"Create optimized Chroma vectorstore with metadata indexing\"\"\"\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "\n",
        "    for job in job_data:\n",
        "        doc = Document(\n",
        "            page_content=job[\"Job_txt\"],\n",
        "            metadata={\n",
        "                \"job_id\": str(uuid.uuid4()),\n",
        "                \"title\": job[\"job-title\"],\n",
        "                \"company\": job.get(\"company\", \"\"),\n",
        "                \"location\": job.get(\"location\", \"\"),\n",
        "                \"industry\": job.get(\"industry\", \"\"),\n",
        "                \"posted_date\": job.get(\"posted-date\", \"\")\n",
        "            }\n",
        "        )\n",
        "        documents.append(doc)\n",
        "        metadatas.append(doc.metadata)\n",
        "\n",
        "    return Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=persist_dir,\n",
        "        collection_metadata={\"hnsw:space\": \"cosine\"}  # Optimized similarity metric\n",
        "    )\n",
        "\n",
        "# Enhanced Retrieval System\n",
        "def create_retrieval_chain(vectorstore, llm):\n",
        "    \"\"\"Create optimized retrieval chain with contextual compression\"\"\"\n",
        "    base_retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 30}  # Retrieve more for reranking\n",
        "    )\n",
        "\n",
        "    # Contextual compression for better relevance\n",
        "    compressor = LLMChainExtractor.from_llm(llm)\n",
        "    return ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=base_retriever,\n",
        "        search_kwargs={\"k\": 20}  # Final number of results\n",
        "    )\n",
        "\n",
        "# Job Matching Pipeline\n",
        "def job_matching_pipeline(cv_text, retriever, embedding_model):\n",
        "    \"\"\"End-to-end matching process\"\"\"\n",
        "    # Generate enriched CV summary\n",
        "    cv_summary = Summarize_job_text(cv_text, prompt_cv)\n",
        "\n",
        "    # Semantic embedding with batch processing\n",
        "    cv_embedding = embedding_model.embed_query(cv_summary)\n",
        "\n",
        "    # Hybrid search with metadata filtering\n",
        "    results = retriever.get_relevant_documents(\n",
        "        cv_summary\n",
        "    )\n",
        "\n",
        "    return process_results(results)\n",
        "\n",
        "def process_results(results):\n",
        "    \"\"\"Enrich and format results with similarity scores\"\"\"\n",
        "    processed = []\n",
        "    for doc in results:\n",
        "        processed.append({\n",
        "            \"title\": doc.metadata[\"title\"],\n",
        "            \"company\": doc.metadata[\"company\"],\n",
        "            \"location\": doc.metadata[\"location\"],\n",
        "            \"match_score\": doc.metadata.get(\"similarity_score\", 0),\n",
        "            \"summary\": doc.page_content[:500] + \"...\",\n",
        "            \"job_id\": doc.metadata[\"job_id\"]\n",
        "        })\n",
        "    return sorted(processed, key=lambda x: x[\"match_score\"], reverse=True)[:20]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-jgHFP9fPJr"
      },
      "outputs": [],
      "source": [
        "# Usage Example\n",
        "\n",
        "    # Initialize models\n",
        "embedding_model = JinaEmbeddings(\n",
        "    jina_api_key=\"jina_a336488c98c6463180a2494f62739952boWqK6nYZSwFNd-Z6JJJ-Gvs1sqW\",\n",
        "    model_name=\"jina-embeddings-v3\"\n",
        ")\n",
        "\n",
        "# Load data\n",
        "jobs_df = pd.read_csv(\"/content/Jobs_data_new_python.csv\").dropna()\n",
        "\n",
        "# Create vectorstore\n",
        "vectorstore = create_enhanced_vectorstore(\n",
        "    jobs_df.to_dict(\"records\"),\n",
        "    embedding_model\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRYjGX4WgNj6"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEpEQ9apg4Xh"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vSiYTSig_FF"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "# Initialize Groq LLM\n",
        "GROQ_API_KEY = \"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\"\n",
        "llm = ChatGroq(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    model_name=\"llama3-8b-8192\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=4000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBYmZ4zkfVw5"
      },
      "outputs": [],
      "source": [
        "    # Create retrieval chain\n",
        "retriever = create_retrieval_chain(vectorstore, llm)\n",
        "\n",
        "# Process CV\n",
        "recommendations = job_matching_pipeline(cv_text, retriever, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDuWJxE6ko7Y",
        "outputId": "8664c467-c990-4afd-a3d7-7f422f0d360e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'title': 'C# Developer',\n",
              "  'company': 'YO IT CONSULTING',\n",
              "  'location': 'Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT....',\n",
              "  'job_id': 'fb132617-c0bc-4a81-887f-dc4a18d87e09'},\n",
              " {'title': 'Python Full Stack Developer',\n",
              "  'company': 'YO IT CONSULTING',\n",
              "  'location': 'Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT\\n\\nThe provided context does not contain any relevant information to answer the question. The question appears to be asking about a job posting for a Python Full Stack Developer position, but the provided context is a CV of an individual with expertise in AI development, machine learning, and natural language processing....',\n",
              "  'job_id': '19123c92-60b2-44df-a057-8495ce13b8ac'},\n",
              " {'title': 'Développeur Backend - PHP',\n",
              "  'company': 'Elavi Agency',\n",
              "  'location': 'Casablanca, Casablanca-Settat, Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT\\n\\nThe provided context does not seem to be relevant to the question. The question is about a CV, and the context appears to be a job posting for a Backend Developer - PHP role at Elavi Agency....',\n",
              "  'job_id': 'c4f3384a-ba18-43b5-96f4-df183ad18ec5'},\n",
              " {'title': 'Python JavaScript Developer',\n",
              "  'company': 'YO IT CONSULTING',\n",
              "  'location': 'Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT\\n\\nThe provided context does not seem to be relevant to the question. The question is about a CV, and the provided context is about a job posting for a Python JavaScript Developer position at YO IT CONSULTING. There is no connection between the two....',\n",
              "  'job_id': '4366399c-ba0b-4cf6-9f56-35ef6fc72d9e'},\n",
              " {'title': 'Virtual Statistical Validation Engineer H/F',\n",
              "  'company': 'Stellantis',\n",
              "  'location': 'Casablanca, Casablanca-Settat, Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT....',\n",
              "  'job_id': 'a25a902a-14bf-4a5e-8add-5fdf9bdcc284'},\n",
              " {'title': 'Python Developer - Technical Leader',\n",
              "  'company': 'Société Générale - Africa Technologies & Services',\n",
              "  'location': 'Casablanca Metropolitan Area',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT\\n\\nThe provided context does not contain any information that is directly relevant to the question. The context appears to be a job posting for a Python Developer - Technical Leader position at Société Générale - Africa Technologies & Services, and does not mention the skills or experience of the candidate being considered for the position....',\n",
              "  'job_id': '9dad1269-c908-4677-a43c-51b65f5903e0'},\n",
              " {'title': 'API Developer (AWS)',\n",
              "  'company': 'Cnexia',\n",
              "  'location': 'Rabat-Salé-Kénitra, Morocco',\n",
              "  'match_score': 0,\n",
              "  'summary': 'NO_OUTPUT....',\n",
              "  'job_id': '72fad64c-1892-4735-8d40-c616acd908d6'}]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recommendations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
